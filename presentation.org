#    -*- mode: org -*-
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:nil reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1600 reveal_height:900
#+OPTIONS: tex:t toc:nil <:nil timestamp:nil email:t reveal_slide_number:"c/t"
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANS: none
#+REVEAL_THEME: blood
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./presentation.css
#+REVEAL_ROOT: ../reveal.js/

#+TITLE: Identifying Monoids
#+AUTHOR: Ben Deane
#+EMAIL: bdeane@quantlab.com
#+DATE: May 2019

# +REVEAL_HTML: <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
#+REVEAL_HTML: <script type="text/javascript" src="./presentation.js"></script>

* Title slide settings                                             :noexport:
#+BEGIN_SRC emacs-lisp
(setq org-re-reveal-title-slide
(concat "<h2>%t</h2>"
"<h3>Exploiting Compositional Structure in Code</h3>"
"<div class='vertspace2'></div>"
"<img src=\"./wood.png\"/>"
"<p>\\(\\left \\{ \\mathbb{Z}, \\times, 1 \\right \\}\\)</p>"
"<div class='vertspace2'></div>"
"<h4>%a / <a href=\"http://twitter.com/ben_deane\">@ben_deane</a>"
" / C++Now / Aspen, CO / %d</h4>"))
#+END_SRC

* Introduction

** Let's get this out of the way.

A monoid is NOT the same thing as a monad.

#+begin_notes
Some of you already know this. (How many people know all about monoids?)

Some people, online or wherever, may be confused, so I wanted to be clear.

This isn't a talk with a lot of category theory. I'm probably going to say lots
of things that mathematicians will take issue with. I'm going to try ground this
talk in examples.
#+end_notes

** Why

Why are monoids important?

Why is "abstract mathematics" important for programming?

#+begin_notes
(Tell the Simeon story.)

There are people who think this kind of mathematics has no place in programming.
(They don't seem to mind other kinds of maths like linear algebra.) They are
looking at things the wrong way.

When we program, we're doing maths whether we know it or not, and it helps us to
realize that. Composition is the essence of programming architecture. Abstract
algebra is the framework for ideas of composition.

You can program without maths, just like you can bake without knowing chemistry.
But you're doing it anyway. You already have an intuition for it.

Abstract algebra is to programming what calculus is to modelling the real world.
#+end_notes

** A quick code review

#+begin_src c++
void RepositionListItem(int drag_start_idx, int drop_idx) {
  // grab the dragged item
  Item* item = list_items_[drag_start_index];

  // move the rest down
  for (auto i = drag_start_idx + 1; i < list_items_.size(); ++i) {
    list_items_[i-1] = list_items_[i];
  }
  list_items_.pop_back();

  // re-insert the item ("drop" it)
  list_items_.insert(item, list_items_.begin() + drop_idx);
}
#+end_src

#+ATTR_REVEAL: :frag (appear)
 - "This is (obviously?) a rotate."
 - And there's a bug here. Is ~drag_start_idx~ ~>~ ~drop_idx~?

#+begin_notes
What would you suggest for this code review?

What is this code doing?

Do you see any problems?
#+end_notes

** How did we know that?

Expertise.

 - Hard to say how
 - Difference in perception
   - Selectivity of attention
   - Units of perception
   - Unconscious search strategies
 - Speed of processing

#+begin_notes
These are the hallmarks of expertise. A difference in what we pay attention to.
A difference in the chunk size of things we perceive. A difference in the search
patterns we pursue. And a huge speed difference.

Everyone here is a C++ expert. We probably can't really explain how we knew the
problems. You just had a "spidey sense". A C++ novice looking at the code may
not even know where to begin.
#+end_notes

** How brains work

1. See lots of examples with lots of variation. Some variation is relevant,
   some is not.
1. Figure out which variation is relevant, and classify.
1. Trial and error: rinse, repeat...
1. Result after time: "instinctual" expertise

#+ATTR_REVEAL: :frag (appear)
So let's get going.

#+begin_notes
We can say how we got here, because this is what brains do. We don't learn by
applying abstract rules. We learn by this process.

So in this talk, mostly I'm going to show lots of different monoids, so that we
get an intuition about how to recognize them; each brain experiencing this talk
will form its own pattern-recognition pathways. This is what brains do (very
well).

When we have that sense, then it can be useful to fill in more theory.
#+end_notes

** What is a monoid?

A set of values.
 - finite or infinite

A binary operation.
 - closed
 - associative

One special value in the set.
 - the identity

#+begin_notes
Before I show examples, here is some context to understand them.
#+end_notes

* Motivation

#+REVEAL_HTML: <div class='vertspace2'></div>
(In response to post-talk questions about how to "identify your monoids")
#+REVEAL_HTML: <div class='vertspace2'></div>

#+REVEAL_HTML: <blockquote nil><p>&quot;As a writer of a library, or code that someone else will use,<br>
#+REVEAL_HTML: identifying monoids in your code -- in your types and your<br>
#+REVEAL_HTML: operations -- I think is one of the single biggest things<br>
#+REVEAL_HTML: you can do to help users of your library.&quot;</p>
#+REVEAL_HTML: <div></div><div class='author'>-- me, <em>Easy to Use, Hard to Misuse: Declarative Style in C++</em></div></blockquote>

** Why is it important to recognize them?

Architecture.
A means to talk about what's happening in our code.

It's not about functional programming, it's not about abstraction for its own
sake.

Computational sympathy.

** Abstraction

"Being abstract is something profoundly different from being vague... the purpose
of abstraction is not to be vague, but to create a new semantic level in which
one can be absolutely precise."

--EWD

** Design Patterns

Compare Design Patterns.

There are people who get the wrong end of the stick about this. There are people
who, for example, tweet that they're hiring, but say things like "use of Design
Patterns is a negative".

Design Patterns came out in 1995. The implementations in it are of their time.
They represent a Java-style, late 90s, heavy-OO, dynamic dispatch style of
implementation. That's not the lasting legacy of the book.

If we're talking about code and I say to you, "I think we could solve this with
a visitor pattern", I'm not saying "let's open the Gang of Four book and
implement all this machinery to implement dynamic double-dispatch." I'm saying
"I think we could benefit from the ability to easily add behaviours over types."
And we can implement that using all the tools we have in our toolbox and make it
perform arbitrarily well according to our needs.

** The power of abstraction

We can talk about our code's needs rather than how to implement those needs

** Late night coding with Simeon

You only have a binary operation to combine objects
You have a list of objects to combine
Show a raw loop
Realize it's an accumulate - we have a monoid!

** Alex Stepanov

Saw/realised the structure of monoids/semigroups while sick

** Why?

When we recognize monoids, we gain the ability to separate concerns.

If we write code as raw loops, we commingle the business logic with the code
that handles control flow.

Instead of having to learn anew how each computation in our code works, a user
of our code/library just has to learn one idea: that of accumulation. They don't
have to puzzle out what each loop is doing.


* Examples

#+REVEAL_HTML: <div class='vertspace2'></div>
We'll start with the obvious ones

** The Obvious Monoids
#+REVEAL_HTML: <div class='vertspace2'></div>

There's a reason why the default operation of ~accumulate~ is addition.

#+REVEAL_HTML: <div class='vertspace2'></div>

 - \( \left \{ \mathbb{R}, +, 0 \right \} \)
 - \( \left \{ \mathbb{R}, \times, 1 \right \} \)

#+REVEAL_HTML: <div class='vertspace2'></div>

For \(\mathbb{R}\), read also \(\mathbb{Z}\) or \(\mathbb{N}\). (And also
\(\mathbb{C}\)).

#+begin_notes
A grade school child can understand monoids. Notice the three properties:

 - closed operation (it's so obvious with these examples, but it's really important
as we shall see later)
 - associativity: it doesn't matter how we group them
 - there is an identity (and only one)

Notice these are commutative, but commutativity is not required.
#+end_notes

** Addition & Multiplication
#+REVEAL_HTML: <div class='vertspace2'></div>

Cover many things that are "number-like".

 - integers (approximated by ~int~ etc)
 - real numbers (approximated by ~float~ or ~double~)
 - complex numbers
 - vectors (in the mathematical sense)
 - matrices

We can use (almost) any of these with ~accumulate~ (or fold expressions)\\
and ~plus~ or ~multiplies~.

#+begin_notes
In C++ of course, we normally approximate all these things with finite datatypes.

A complex number is just a pair in the complex plane with memberwise addition.
Recall for multiplication we multiply the magnitudes (moduli) and sum the angles
(arguments). The identity is therefore (1, 0).

For vectors we can do memberwise addition, but we can't do multiplication: the
dot product isn't closed, and the cross product has no identity since it always
produces a vector perpendicular to the two inputs.

Matrix addition is memberwise. Note matrix multiplication isn't commutative.
#+end_notes

** ~min~ and ~max~
#+REVEAL_HTML: <div class='vertspace2'></div>

It's clear that ~max~ is a monoid on positive numbers:

\( \left \{ \mathbb{Z^+}, max, 0 \right \} \)

#+REVEAL_HTML: <div class='vertspace2'></div>

~min~ is less clear mathematically...

\( \left \{ \mathbb{Z}, min, ? \right \} \)

... but we can often use ~numeric_limits<T>::max~ as the identity.

#+begin_notes
Again for Z, read "anything numeric".

Mirror situations apply for dealing with negative numbers.
#+end_notes

** Boolean values: AND and OR
#+REVEAL_HTML: <div class='vertspace2'></div>
 \( \left \{ \{true, false\}, \land, true \right \} \)
#+begin_src c++
template <typename... Args>
bool all(Args... args) { return (... && args); }
#+end_src

#+REVEAL_HTML: <div class='vertspace2'></div>
 \( \left \{ \{true, false\}, \lor, false \right \} \)
#+begin_src c++
template <typename... Args>
bool any(Args... args) { return (... || args); }
#+end_src

#+begin_notes
C++ allows us to use logical AND and logical OR in unary folds.

The value for an empty pack with AND is ~true~.

The value for an empty pack with OR is ~false~.
#+end_notes

** Boolean values: XOR
#+REVEAL_HTML: <div class='vertspace2'></div>
\( \left \{ \{true, false\}, \oplus, false \right \} \)

| A       | B       | Result  |
|---------+---------+---------|
| ~false~ | ~false~ | ~false~ |
| ~false~ | ~true~  | ~true~  |
| ~true~  | ~false~ | ~true~  |
| ~true~  | ~true~  | ~false~ |

#+REVEAL_HTML: <div class='vertspace2'></div>
Note: exclusive-or on ~bool~ is ~operator!=~

#+begin_notes
For XOR, the identity is ~false~ as we can see from the truth table.

In C++, we don't have logical XOR (~^^~?) but we do have bitwise XOR.
#+end_notes

* Code Interlude

#+REVEAL_HTML: <div class='vertspace2'></div>
Recognizing accumulation-style algorithms

** Code: the obvious algorithms
#+REVEAL_HTML: <div class='vertspace2'></div>

The following algorithms are almost a dead giveaway:

 - ~accumulate~, ~reduce~
 - basically, all the algorithms in ~<numeric>~
 - fold expressions

** ~<algorithm>~: the other "usual suspects"
#+REVEAL_HTML: <div class='vertspace2'></div>

Suspect a monoid whenever you find yourself using the following algorithms:

 - ~all_of~, ~any_of~, ~none_of~
 - (therefore also ~find~ and friends)
 - ~min_element~, ~max_element~, ~minmax_element~
 - ~count~, ~count_if~
 - ~equal~

** Useful reformulations of ~accumulate~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template <class InputIt, class Size, class T, class BinaryOp>
constexpr auto accumulate_n(InputIt first, Size n, T init, BinaryOp op)
    -> std::pair<T, InputIt> {
  for (; n > 0; --n, ++first) {
    init = op(std::move(init), *first);
  }
  return {init, first};
}
#+end_src

The standard library has some ~*_n~ algorithms; it should have more.

#+begin_notes
Note the principle of useful return here: we also return the iterator we've
reached.

Basically all the algorithms in the standard library should be available in two
forms: iterator-pair form and iterator, count form.

This idea is in EoP: some algorithms may be more efficient in the count form or
may provide more useful building blocks in that form.
#+end_notes

** Useful reformulations of ~accumulate~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template <class InputIt, class T, class BinaryOp>
constexpr T accumulate_iter(InputIt first, InputIt last, T init, BinaryOp op) {
  for (; first != last; ++first) {
    init = op(std::move(init), first);
  }
  return init;
}
#+end_src

Pass the iterator to the ~op~ /undereferenced/.

#+begin_notes
The only difference here from the standard ~accumulate~ is the absence of a ~*~.
This is a formulation of ~accumulate~ that I used for the code experiments in my
2016 talk "accumulate: Exploring an Algorithmic Empire".

In C++2014 there were 90 standard algorithms. Using this formulation of
accumulate and some jiggery-pokery I was able to implement 77 of them.
#+end_notes

* More Examples

#+REVEAL_HTML: <div class='vertspace2'></div>
Because brains learn by seeing lots of variations.

** Strings
#+REVEAL_HTML: <div class='vertspace2'></div>

 - ~string~
 - ~operator+~ (concatenation)
 - empty string

#+REVEAL_HTML: <div class='vertspace2'></div>
Strings form a monoid under concatenation.\\
The identity is the empty string.

#+begin_notes
This is sometimes called "the free monoid". Note that it's not commutative. It's
"free" in the sense that it's the "generic" monoid with only the basic rules and
no other structure applied.
#+end_notes

** String-ish applications
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
std::vector<T> v{1, 2, 3, 4, 5};

std::accumulate(
    std::cbegin(v), std::cend(v), std::ref(std::cout),
    [](auto &os, auto &elem) -> decltype(auto) { return os.get() << elem; });
#+end_src

Here, ~cout~ is acting like the accumulating string.

Compare: range solution with projection function.

** String-ish applications
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
std::string url_base = "https://example.com/?";
std::map<std::string, std::string> url_args {{"alpha", "able"},
                                             {"bravo", "baker"}};

join(std::cbegin(url_args), std::cend(url_args),
     std::back_inserter(url_base), '&',
     [] (const auto& p) {
       const auto& [key, val] = p;
       return key + '=' + val;
     });
#+end_src

We accumulate the query arguments into the url.

* Optional

Make a semigroup into a monoid!

* Sets and Maps

Monoids on their value types

* Structs, pairs and tuples

Product types are monoidal if their constituents are
This is useful for accumulating effects
cf. Writer monad
http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html

* Functions

Monoidal on their outputs
cf. maps

* Tree structures

"Normal" monoids have two operations:
 - one for combining with identity
 - one for combining with value

If we look at a tree structure as a sum type, we can extend this to:
 - one for combining each type of value

And we can run functions over tree structures, accumulating the values. Just
like std::accumulate, this gives us the power to separate the meaning of what
we're doing from the control flow of how we're doing it.

Most of the algorithms in the STL deal with linear sequences. And that's most of
what we handle. But quite often we handle treelike structures. It's much more
likely then that we'll fail to see the essential computation that's going on and
fall back on a raw loop because we think algorithms can't deal with what we're
doing. If we think in terms of monoids, we can get that separation of control
flow from logic and we can often use an accumulate-like algorithm to separate
the control flow.

* Endofunctions

Functions from A to A
Processes evolving in time
std::iota
std::iterate
ranges

Examples:

 - Eller's algo for maze generation - plain std::accumulate (linear data dependence)
   or partial_sum for intermediate output (good range example?)

 - RNG - LCG is a linear recurrence relation f :: a -> a
   represent as a matrix -> function composition is raising to nth power (log n)
   can "fast forward" RNG in log time because it's a monoid
https://www.nayuki.io/page/fast-skipping-in-a-linear-congruential-generator

Research:

https://meetingcpp.com/blog/items/ranges-for-numerical-problems-402.html
https://www.youtube.com/watch?v=13r9QY6cmjc
http://people.math.gatech.edu/~ecroot/recurrence_notes2.pdf

* Futures

when_any and never
UI applications

* Stronger than monoids
Commutativity
Existence of an inverse

* Accumulate vs reduce

Reduce requires commutativity for vectorization
Data-level parallelism at war with function-level parallelism
(parallelism vs concurrency)

* Balanced reduction

* Incremental computation

* Bigger applications

** Serialization
Monoid-like, but with varying types

** Profiling
Another kind of serialization

** Statistics
Keeping a mean
Keeping a median
Top n
Histograms

** Probabilistic algorithms
"Fantastic algorithms and where to find them"
"Add ALL the things!"

Hyperloglog
Count min-sketch

** Config

JSON objects, databases, configuration blobs, sets of command-line flags
Protocol buffers

We "reinvent" monoids all the time without realizing it! Most of the time when
we deal with these kinds of things, we don't think about their monoidal nature.

https://mail.haskell.org/pipermail/haskell-cafe/2009-January/053709.html

** Parsers

Parsers are monoids under alternation. The identity is the parser that always
fails. This is a common pattern if you have an operation that can fail.

cf. when_any
cf. optional

** Monoid homomorphisms

A function that preserves the monoid structure. If A and B are monoids under
some operations, then f :: A -> B is a monoid homomorphism if it preserves the
structure.

e.g.
 - strings are monoids under concatenation.
 - integers are monoids under addition.

string length is a monoid homomorphism.

** Why use monoid homomorphisms?

- to get into a space that is easier to reason about
- to be able to do more
- for performance
- all of the above

** For perf?

We're always doing things for performance reasons of course.
We're often computing things in a "different space" for perf reasons.

e.g.

In vector spaces, we can avoid square roots when computing magnitudes because we
can compare to a precomputed magnitude in "squared space" instead.

In vector spaces, we measure angles between vectors by comparing with
precomputed cosine constants rather than doing an inverse trig function.

** Ofuscated example

#+begin_src c
main(n){float r,i,R,I,b;for(i=-1;i<1;i+=.06,puts(""))for(r=-2;I=i,(R=r)<1;
r+=.03,putchar(n+31))for(n=0;b=I*I,26>n++&&R*R+b<4;I=2*R*I+i,R=R*R-b+r);}
#+end_src

#+begin_notes
We're so used to working in "a more computationally efficient space".

Maybe you can guess what this does? Hint: it's from the mid-90s.

Check out the "R*R+b<4" part.
#+end_notes

** Monoid Homomorphism example

The usual example is string -> int (length).

Sometimes the monoid is buried. Often the "surface" monoid is a monoid we can't
express very well in C++. Like function composition.

LCG example.

** Regular expressions

http://blog.sigfpe.com/2009/01/fast-incremental-regular-expression.html

** Tournaments

** Diagrams


* My favourite quote

"Discovery consists of seeing what everybody has seen, and thinking what nobody
has thought."

Albert Szent-Györgyi. (Hungarian Nobel Laureate in Medicine, 1937)

#+begin_notes
I hope that after this talk you can look at your code in a new way and think
what you have not thought before.
#+end_notes

* References
/Cultivating Instinct/ Katrina Owen
https://www.youtube.com/watch?v=Q1Tlo4VnQrA
