#    -*- mode: org -*-
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:nil reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1600 reveal_height:900
#+OPTIONS: tex:t toc:nil <:nil timestamp:nil email:t reveal_slide_number:"c/t"
#+REVEAL_MARGIN: 0.1
#+REVEAL_MIN_SCALE: 0.5
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_TRANS: none
#+REVEAL_THEME: blood
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./presentation.css
#+REVEAL_ROOT: ../reveal.js/

#+TITLE: Identifying Monoids
#+AUTHOR: Ben Deane
#+EMAIL: bdeane@quantlab.com
#+DATE: May 2019

# +REVEAL_HTML: <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
#+REVEAL_HTML: <script type="text/javascript" src="./presentation.js"></script>

* Title slide settings                                             :noexport:
#+BEGIN_SRC emacs-lisp
(setq org-re-reveal-title-slide
(concat "<h2>%t</h2>"
"<h3>Exploiting Compositional Structure in Code</h3>"
"<div class='vertspace2'></div>"
"<img src=\"./wood.png\"/>"
"<p>\\(\\left \\{ \\mathbb{Z}, \\times, 1 \\right \\}\\)</p>"
"<div class='vertspace2'></div>"
"<h4>%a / <a href=\"http://twitter.com/ben_deane\">@ben_deane</a>"
" / C++Now / Aspen, CO / %d</h4>"))
#+END_SRC

* Introduction/Motivation

#+REVEAL_HTML: <div class='vertspace2'></div>
(In response to post-talk questions about how to "identify your monoids")
#+REVEAL_HTML: <div class='vertspace2'></div>

#+REVEAL_HTML: <blockquote nil><p>&quot;As a writer of a library, or code that someone else will use,<br>
#+REVEAL_HTML: identifying monoids in your code -- in your types and your<br>
#+REVEAL_HTML: operations -- I think is one of the single biggest things<br>
#+REVEAL_HTML: you can do to help users of your library.&quot;</p>
#+REVEAL_HTML: <div></div><div class='author'>-- me, <em>Easy to Use, Hard to Misuse: Declarative Style in C++</em></div></blockquote>

#+begin_notes
It's also one of the best things you can do to help your own thinking about
library architecture and the macro structure of your code.

When we recognize monoids, we gain the ability to separate concerns.

If we write code as raw loops, we commingle the business logic with the code
that handles control flow.

Instead of having to learn anew how each computation in our code works, a user
of our code/library just has to learn one idea: that of accumulation. They don't
have to puzzle out what each loop is doing.
#+end_notes

** Preliminaries

#+REVEAL_HTML: <div class='vertspace2'></div>
Let's get this out of the way.

#+REVEAL_HTML: <div class='vertspace2'></div>
A monoid is NOT the same thing as a monad.

#+begin_notes
Some of you already know this. (How many people know all about monoids?)

Some people, online or wherever, may be confused, so I wanted to be clear.

This isn't a talk with a lot of category theory. I'm probably going to say lots
of things that mathematicians will take issue with. I'm going to try ground this
talk in examples.

Also, I will be interchangeably using the terms: fold, accumulate, sometimes
reduce.
#+end_notes

** Why

#+REVEAL_HTML: <div class='vertspace2'></div>
Why are monoids important?

Why is "abstract mathematics" important for programming?

#+begin_notes
(Tell the Simeon story.)

Maths : Programming :: Chemistry : Baking

You can program without maths, just like you can bake without knowing chemistry.
But you're doing it anyway. You already have an intuition for it.

When we program, we're doing maths whether we know it or not, and it helps us to
realize that. Composition is the essence of programming architecture. Abstract
algebra is the framework for ideas of composition.

Abstract algebra is to programming what calculus is to modelling the real world.
#+end_notes

** Abstractions

#+REVEAL_HTML: <div class='vertspace2'></div>
#+REVEAL_HTML: <blockquote nil><p>&quot;Being abstract is something profoundly different from being<br>
#+REVEAL_HTML: vague... the purpose of abstraction is not to be vague,<br>
#+REVEAL_HTML: but to create a new semantic level<br>in which one can be absolutely precise.&quot;</p>
#+REVEAL_HTML: <div></div><div class='author'>-- EWD</div></blockquote>

#+begin_notes
Monoids embody a fundamental shape of computation. So they're a tool for talking
about and thinking about code.

Compare Design Patterns. The book came out in 1995. The implementations
in it are of their time. They represent a Java-style, late 90s, heavy-OO,
dynamic dispatch style of implementation. That's not the lasting legacy of the
book.

"I think we could solve this with a visitor pattern" doesn't mean "let's open
the Gang of Four book and implement all this machinery to implement dynamic
double-dispatch." It means something like "I recognize the shape of this code,
and I think we could benefit from the ability to easily add behaviours over
types."
#+end_notes

** A quick code review

#+begin_src c++
void RepositionListItem(int drag_start_idx, int drop_idx) {
  // grab the dragged item
  Item* item = list_items_[drag_start_index];

  // move the rest down
  for (auto i = drag_start_idx + 1; i < list_items_.size(); ++i) {
    list_items_[i-1] = list_items_[i];
  }
  list_items_.pop_back();

  // re-insert the item ("drop" it)
  list_items_.insert(item, list_items_.begin() + drop_idx);
}
#+end_src

#+ATTR_REVEAL: :frag (appear)
 - "This is (obviously?) a rotate."
 - And there's a bug here. Is ~drag_start_idx~ ~>~ ~drop_idx~?

#+begin_notes
What would you suggest for this code review?

What is this code doing?

Do you see any problems?
#+end_notes

** How did we know that?

#+REVEAL_HTML: <div class='vertspace2'></div>
Expertise.

 - Hard to say how
 - Difference in perception
   - Selectivity of attention
   - Units of perception
   - Unconscious search strategies
 - Speed of processing

#+begin_notes
These are the hallmarks of expertise. A difference in what we pay attention to.
A difference in the chunk size of things we perceive. A difference in the search
patterns we pursue. And a huge speed difference.

Everyone here is a C++ expert. We probably can't really explain how we knew the
problems. You just had a "spidey sense". A C++ novice looking at the code may
not even know where to begin.
#+end_notes

** How brains work

#+REVEAL_HTML: <div class='vertspace2'></div>
1. See lots of examples with lots of variation. Some variation is relevant,
   some is not.
1. Figure out which variation is relevant, and classify.
1. Trial and error: rinse, repeat...
1. Result after time: "instinctual" expertise

/Cultivating Instinct/ Katrina Owen
https://www.youtube.com/watch?v=Q1Tlo4VnQrA

#+begin_notes
We can say how we got here, because this is what brains do. We don't learn by
applying abstract rules. We learn by this process.

So in this talk, mostly I'm going to show lots of different monoids, so that we
get an intuition about how to recognize them; each brain experiencing this talk
will form its own pattern-recognition pathways. This is what brains do (very
well).

When we have that sense, then it can be useful to fill in more theory.
#+end_notes

** What is a monoid?

#+REVEAL_HTML: <div class='vertspace2'></div>
A set of values.
 - finite or infinite

A binary operation.
 - closed
 - associative

One special value in the set.
 - the identity

#+ATTR_REVEAL: :frag (appear)
So let's see some examples.

#+begin_notes
Before I show examples, here is some context to understand them.

I want to build a sense for spotting them in the wild and an intuition for
knowing when they might be lurking. So let's get to examples.
#+end_notes

* Examples

#+REVEAL_HTML: <div class='vertspace2'></div>
We'll start with the obvious ones

** The Obvious Monoids
#+REVEAL_HTML: <div class='vertspace2'></div>

There's a reason why the default operation of ~accumulate~ is addition.

#+REVEAL_HTML: <div class='vertspace2'></div>

 - \( \left \{ \mathbb{R}, +, 0 \right \} \)
 - \( \left \{ \mathbb{R}, \times, 1 \right \} \)

#+REVEAL_HTML: <div class='vertspace2'></div>

For \(\mathbb{R}\), read also \(\mathbb{Z}\) or \(\mathbb{N}\). (And also
\(\mathbb{C}\)).

#+begin_notes
A grade school child can understand monoids. Notice the three properties:

 - closed operation (it's so obvious with these examples, but it's really important
as we shall see later)
 - associativity: it doesn't matter how we group them
 - there is an identity (and only one)

Notice these are commutative, but commutativity is not required.
#+end_notes

** Addition & Multiplication
#+REVEAL_HTML: <div class='vertspace2'></div>

Cover many things that are "number-like".

 - integers (approximated by ~int~ etc)
 - real numbers (approximated by ~float~ or ~double~)
 - complex numbers
 - vectors (in the mathematical sense)
 - matrices

We can use (almost) any of these with ~accumulate~ (or fold expressions)\\
and ~plus~ or ~multiplies~.

#+begin_notes
In C++ of course, we normally approximate all these things with finite datatypes.

A complex number is just a pair in the complex plane with memberwise addition.
Recall for multiplication we multiply the magnitudes (moduli) and sum the angles
(arguments). The identity is therefore (1, 0).

For vectors we can do memberwise addition, but we can't do multiplication: the
dot product isn't closed, and the cross product has no identity since it always
produces a vector perpendicular to the two inputs.

Matrix addition is memberwise. Note matrix multiplication isn't commutative.
#+end_notes

** ~min~ and ~max~
#+REVEAL_HTML: <div class='vertspace2'></div>

It's clear that ~max~ is a monoid on positive numbers:

\( \left \{ \mathbb{Z^+}, max, 0 \right \} \)

#+REVEAL_HTML: <div class='vertspace2'></div>

~min~ is less clear mathematically...

\( \left \{ \mathbb{Z}, min, ? \right \} \)

... but we can often use ~numeric_limits<T>::max~ as the identity.

#+begin_notes
Again for Z, read "anything numeric".

Mirror situations apply for dealing with negative numbers.
#+end_notes

** Boolean values: AND and OR
#+REVEAL_HTML: <div class='vertspace2'></div>
 \( \left \{ \{true, false\}, \land, true \right \} \)
#+begin_src c++
template <typename... Args>
bool all(Args&&... args) { return (... && args); }
#+end_src

#+REVEAL_HTML: <div class='vertspace2'></div>
 \( \left \{ \{true, false\}, \lor, false \right \} \)
#+begin_src c++
template <typename... Args>
bool any(Args&&... args) { return (... || args); }
#+end_src

#+begin_notes
C++ allows us to use logical AND and logical OR in unary folds.

The value for an empty pack with AND is ~true~.

The value for an empty pack with OR is ~false~.
#+end_notes

** Boolean values: XOR
#+REVEAL_HTML: <div class='vertspace2'></div>
\( \left \{ \{true, false\}, \oplus, false \right \} \)

| A       | B       | Result  |
|---------+---------+---------|
| ~false~ | ~false~ | ~false~ |
| ~false~ | ~true~  | ~true~  |
| ~true~  | ~false~ | ~true~  |
| ~true~  | ~true~  | ~false~ |

#+REVEAL_HTML: <div class='vertspace2'></div>
Note: exclusive-or on ~bool~ is ~operator!=~

#+begin_notes
For XOR, the identity is ~false~ as we can see from the truth table.

In C++, we don't have logical XOR (~^^~?) but we do have bitwise XOR.
#+end_notes

* Code Interlude

#+REVEAL_HTML: <div class='vertspace2'></div>
Recognizing accumulation-style algorithms

** Code: the obvious algorithms
#+REVEAL_HTML: <div class='vertspace2'></div>

The following algorithms are almost a dead giveaway:

 - ~accumulate~, ~reduce~
 - basically, all the algorithms in ~<numeric>~
 - fold expressions

** ~<algorithm>~: the other "usual suspects"
#+REVEAL_HTML: <div class='vertspace2'></div>

Suspect a monoid whenever you find yourself using the following algorithms:

 - ~all_of~, ~any_of~, ~none_of~
 - (therefore also ~find~ and friends)
 - ~min_element~, ~max_element~, ~minmax_element~
 - ~count~, ~count_if~

#+begin_notes
#+end_notes

** Useful reformulations of ~accumulate~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template <class InputIt, class Size, class T, class BinaryOp>
constexpr auto accumulate_n(InputIt first, Size n, T init, BinaryOp op)
    -> std::pair<T, InputIt> {
  for (; n > 0; --n, ++first) {
    init = op(std::move(init), *first);
  }
  return {init, first};
}
#+end_src

The standard library has some ~*_n~ algorithms; it should have more.

#+begin_notes
Note the principle of useful return here: we also return the iterator we've
reached.

Basically all the algorithms in the standard library should be available in two
forms: iterator-pair form and iterator, count form.

This idea is in EoP: some algorithms may be more efficient in the count form or
may provide more useful building blocks in that form.

I've used this in sliding-window type calculations, where you know the size of
the window.
#+end_notes

** Useful reformulations of ~accumulate~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template <class InputIt, class T, class BinaryOp>
constexpr T accumulate_iter(InputIt first, InputIt last, T init, BinaryOp op) {
  for (; first != last; ++first) {
    init = op(std::move(init), first);
  }
  return init;
}
#+end_src

Pass the iterator to the ~op~ /undereferenced/.

#+begin_notes
The only difference here from the standard ~accumulate~ is the absence of a ~*~.
This is a formulation of ~accumulate~ that I used for the code experiments in my
2016 talk "accumulate: Exploring an Algorithmic Empire".

In C++2014 there were 90 standard algorithms. Using this formulation of
accumulate and some jiggery-pokery I was able to implement 77 of them.
#+end_notes

* More Examples

#+REVEAL_HTML: <div class='vertspace2'></div>
Because brains learn by seeing lots of variations.

** Strings
#+REVEAL_HTML: <div class='vertspace2'></div>

 - ~string~
 - ~operator+~ (concatenation)
 - empty string

#+REVEAL_HTML: <div class='vertspace2'></div>
Strings form a monoid under concatenation.\\
The identity is the empty string.

#+begin_notes
This is sometimes called "the free monoid". Note that it's not commutative. It's
"free" in the sense that it's the "generic" monoid with only the basic rules and
no other structure applied.
#+end_notes

** String-ish applications
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
std::vector<T> v{1, 2, 3, 4, 5};

std::accumulate(
    std::cbegin(v), std::cend(v), std::ref(std::cout),
    [](auto &os, auto &elem) -> decltype(auto) { return os.get() << elem; });
#+end_src

Here, ~cout~ is acting like the accumulating string.

#+begin_notes
The actual code is making some concessions to performance. We can't just write
(string + string + string...) because we don't have efficient ways to look
through the copying of strings.

But I like to think about this in a way that highlights the monoidal structure.
What's really happening is that we're using a projection function on elements to
turn them into strings, and then we're accumulating a string in the world.
#+end_notes

** String-ish applications
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
std::string url_base = "https://example.com/?";
std::map<std::string, std::string> url_args {{"alpha", "able"},
                                             {"bravo", "baker"}};

join(std::cbegin(url_args), std::cend(url_args),
     std::back_inserter(url_base), '&',
     [] (const auto& p) {
       const auto& [key, val] = p;
       return key + '=' + val;
     });
#+end_src

We accumulate the query arguments into the url.

** Joining string-ish things

#+begin_src c++
template <typename InputIt, typename OutputIt, typename T, typename Projection>
OutputIt join(InputIt first, InputIt last,
              OutputIt dest,
              const T& delimiter,
              Projection&& proj);
#+end_src

See also: ~std::experimental::ostream_joiner~, ~ranges::view::join~.

#+begin_notes
With ranges we can also pipe through a projection function quite easily. The
monoidal structure of the code becomes a bit clearer, because the range
machinery provides that lazy conversion.
#+end_notes

** Animations: a monoidal thought experiment

Consider an animation library.

What is an animation?
 - a series of keyframes?
 - a series of blends (curves?) between them?
 - a function from time to position?

How can we compose animations?
 - by pointwise operation
 - by sequencing

#+begin_notes
Let's take what we've seen so far and do a thought experiment: how could we
design API elements for an animation library?

Think about 1D animation to make it simple.

Compose by operation: any monoidal operation! Recall: recognize monoids by
thinking about what the identity is. (In this case the same-length animation
that is all "zeroes").

Compose by sequencing: like a string. Again, the identity is the zero-length
animation.
#+end_notes

* Going further

#+REVEAL_HTML: <div class='vertspace2'></div>
We've seen:
 - "primitive" monoids (on "number-like" things)
 - the free monoid (concatenation)

#+REVEAL_HTML: <div class='vertspace2'></div>
Let's look at composition.

** Containers
#+REVEAL_HTML: <div class='vertspace2'></div>

[[./pointwise_vector.svg]]

#+REVEAL_HTML: <div class='vertspace2'></div>
A container is a monoid on its ~value_type~.

#+begin_notes
Imagine having two maps that you want to combine.

In the first map, a key has a given value. In the second map, the same key has
another value. To combine the maps, we can apply the monoid operation on the two
values to get the resultant mapped value in the output.
#+end_notes

** Maps
#+REVEAL_HTML: <div class='vertspace2'></div>
A ~map~ is a monoid on its ~mapped_type~.

#+begin_src c++
std::map<std::string, int> jan_hours{{"Alice", 80},
                                     {"Bob", 90}};
std::map<std::string, int> feb_hours{{"Bob", 90},
                                     {"Charlie", 70}};

std::map<std::string, int> total_hours = ...;
// {"Alice", 80}, {"Bob", 180}, {"Charlie", 70}
#+end_src

As maps, so (pure) functions.

#+begin_notes
It's easy to see how to compose maps where the keys are the same.

Notice the importance of the identity here: Alice worked the identity number of
hours in Feb, Charlie worked the identity number of hours in Jan. If we didn't
have an identity, this wouldn't work.

If we write pure functions, outputs depend only on inputs, so we can think of
those functions as maps from input type to output type.
#+end_notes

** Product types: memberwise monoidal
#+REVEAL_HTML: <div class='vertspace2'></div>
~struct~, ~pair~, ~tuple~

#+begin_src c++
using modulus_t = double;
using argument_t = double;
using polar_complex_number_t = std::pair<modulus_t, argument_t>;

using computation_t = auto (*) (int) -> int;
using profile_data_t = std::pair<computation_t, chrono::nanoseconds>;
#+end_src

#+begin_notes
Two examples here: the first shows a pair of the same type where the monoidal
operation is different. (Consider complex number multiplication.)

The second shows two differenty types, so necessarily the monoids are different.
Here the monoid for the function could be composition (more on that later), and
the monoid for the profiled time is addition.
#+end_notes

** Sets
#+REVEAL_HTML: <div class='vertspace2'></div>
(Mathematical) sets are monoidal in another way: by intersection and union.

#+REVEAL_HTML: <div class='vertspace2'></div>
\( \left \{ \{sets\}, \cup, \varnothing \right \} \)

\( \left \{ \{sets\}, \cap, \mathbb{U} \right \} \)

#+begin_notes
The empty set is usually easy to code.

The universe (all possible sets) is usually more difficult...

One of the applications here is collecting properties, arguments, etc.
#+end_notes

* Monoidal configuration
#+REVEAL_HTML: <div class='vertspace2'></div>
Let's look at another common application of several monoidal structures we've seen
so far.

#+begin_notes
Let's take a brief aside to look at a more concrete illustration of monoids.
#+end_notes

** Configuration
#+REVEAL_HTML: <div class='vertspace2'></div>
 - JSON objects
 - configuration blobs
 - sets of command-line flags
 - serialization formats (e.g. Protocol buffers)

#+begin_notes
All of these things commonly re-invent monoidal structures without really
realizing it.

We commonly merge these things.
 - overlaying later, higher priority values on earlier ones (replacement)
 - concatenating containers of things
 - recursive merging

These are all monoidal operations. However, the API we present seldom recognizes
the presence of the monoid or allows us to parameterize it fully.
#+end_notes

** Protocol Buffers: monoids in disguise
"Normally, an encoded message would never have more than one instance of a
non-repeated field. However, parsers are expected to handle the case in which
they do. For numeric types and strings, if the same field appears multiple
times, the parser accepts the last value it sees. For embedded message fields,
the parser merges multiple instances of the same field, as if with the
~Message::MergeFrom~ method – that is, all singular scalar fields in the latter
instance replace those in the former, singular embedded messages are merged, and
repeated fields are concatenated."

https://developers.google.com/protocol-buffers/docs/encoding

#+begin_notes
Here's the Protobuf description of how merging works - it's a monoid.

In fact it's a couple of monoids. The set union monoid is here, and so is the
last monoid (right-biased replacement).

And the monoids are composed together.
#+end_notes

** Protocol Buffers: monoids in disguise
"As mentioned above, elements in a message description can be labeled optional.
... If the default value is not specified for an optional element, a
type-specific default value is used instead"

https://developers.google.com/protocol-buffers/docs/proto

#+begin_notes
And here's an example of an identity element.
#+end_notes

* Code Interlude

#+REVEAL_HTML: <div class='vertspace2'></div>
Identity problems.

** Value type problems
#+REVEAL_HTML: <div class='vertspace2'></div>
Usually we would want an identity to be provided by a type's default
constructor.

But sometimes, there is no good identity.

#+begin_src c++
struct color { ... };
#+end_src

Usually for one of two reasons:
 - real-world values don't have defaults
 - different identities are required for different operations

#+begin_notes
Often occurs in values representing things in the real world. Color has several
perfectly suitable operations we can use to combine. But no one identity?

Clue to this: no good value choice for a default constructor.

Or: identity depends on operation, and default construction only has one
implementation.

This is a surmountable problem. You could use for example traits classes.
#+end_notes

** Identity problems
#+REVEAL_HTML: <div class='vertspace2'></div>
Sometimes, an operation is closed and associative, but really has no identity.

#+REVEAL_HTML: <div class='vertspace2'></div>
Or, your datatype might not be able to express the identity.\\
(You crafted it that way for safety in other areas.)

#+REVEAL_HTML: <div class='vertspace2'></div>
What to do?

#+begin_notes
This is a more serious problem. The second case is perhaps more likely.

We generally want to use strong types safely. It is often the case that an
identity is some kind of sentinel value like a null pointer or an empty string,
and you don't want to deal with it in most of the code.

Sometimes you just want to use that identity value in one place where you want
the monoidal property.
#+end_notes

** ~std::optional~ to the rescue
#+REVEAL_HTML: <div class='vertspace2'></div>
Providing a sentinel value that you can use as an identity is what
~std::optional~ does.

#+begin_src c++
template <typename Operation, typename T>
auto monoid_op = [](const std::optional<T>& x, const std::optional<T>& y)
    -> std::optional<T> {
  if (x == std::nullopt) return y;
  if (y == std::nullopt) return x;

  return Operation{}(*x, *y);
};
#+end_src

** ~std::optional~ as a Monoid
#+REVEAL_HTML: <div class='vertspace2'></div>

If ~T~ is a semigroup, then ~std::optional<T>~ is a monoid.

#+REVEAL_HTML: <div class='vertspace2'></div>
Note: a monoid is a semigroup.

* Monoidal statistics
#+REVEAL_HTML: <div class='vertspace2'></div>
Computation of statistics is almost always monoidal.

#+REVEAL_HTML: <div class='vertspace2'></div>
Recognizing and exploiting monoidal properties allows us to distribute
computations.

** Simply summing (counting) things
#+REVEAL_HTML: <div class='vertspace2'></div>

[[./distributed_count1.svg]]

#+REVEAL_HTML: <div class='vertspace2'></div>
Monoids are closed.

#+begin_notes
The property of being closed is the key to using bounded space.
#+end_notes

** Simply summing (counting) things
#+REVEAL_HTML: <div class='vertspace2'></div>

[[./distributed_count2a.svg]]

#+REVEAL_HTML: <div class='vertspace2'></div>
Monoids are associative.

#+begin_notes
The property of associativity is the key to distribution.

This is distribution over hardware...
#+end_notes

** Simply summing (counting) things
#+REVEAL_HTML: <div class='vertspace2'></div>

[[./distributed_count2b.svg]]

#+REVEAL_HTML: <div class='vertspace2'></div>
Monoids are associative.

#+begin_notes
... and here is distribution over time.

If we keep the sums at each level of the tree here then we can query any time
period in logarithmic time.
#+end_notes

** Simply summing (counting) things
#+REVEAL_HTML: <div class='vertspace2'></div>

[[./distributed_count3.svg]]

#+REVEAL_HTML: <div class='vertspace2'></div>
Monoids have an identity.

#+begin_notes
The existence of an identity is the key to flexibility in management.

Zeroes don't matter => it doesn't matter
#+end_notes

** A few statistical monoids
#+REVEAL_HTML: <div class='vertspace2'></div>

#+ATTR_REVEAL: :frag (appear)
 - max and min
 - top N
 - mean
 - histogram

#+begin_notes
Max and min are obvious.

Top N is an easy extension of max/min (i.e. top 1).

Mean is easy too: one way to do it is store the sum and count.

We can imagine how histogram works: it's basically a vector of counts, and we
know how to sum that pointwise.

All of these are composable. We could have a histogram of top Ns, or top N
averages, etc.
#+end_notes

** Fantastic (Monoidal) Algorithms
#+REVEAL_HTML: <div class='vertspace2'></div>

Micholas Ormrod's 2017 CppCon talk "Fantastic Algorithms and Where to Find Them".

https://www.youtube.com/watch?v=YA-nB2wjVcI

 - Heavy hitters
 - Reservoir sampling
 - HyperLogLog

These all have monoidal structure.

#+begin_notes
These are probabilistic algorithms.

They work by keeping relatively small amounts of state, that we know how to
combine with a monoid operation. This is the key to their distribution.
#+end_notes

** HyperLogLog
#+REVEAL_HTML: <div class='vertspace2'></div>

Intuition for HyperLogLog

[[./hyperloglog.svg]]

#+REVEAL_HTML: <div class='vertspace2'></div>
 - we have an ideal hash function
 - we've seen N items
 - the expected "inter-hash" value is \( E(e) = \frac{1}{N+1} \)
 - therefore the expected min value is \( E(e) = \frac{1}{N+1} \)
 - we can recover N from \( \frac{1}{e} - 1 \)

#+begin_notes
This is basically the intuition for HyperLogLog: an ideal hash function is like
a uniformly distributed RNG.

Like all probabilistic algorithms, if you spend more CPU and/or more memory, you
can bound your error more tightly.

In the case of HLL, this means using tricks to effectively compute several
different hashes and store multiple minima in a vector. We do this on several
machines, and we know how to combine these vectors monoidally.

At a very modest cost we can count billions of uniques with say 99% accuracy.
#+end_notes

** Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

Intuition for Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

#+attr_html: :width 600px
[[./count-min_sketch1.svg]]

insert(Alice)

#+begin_notes
With count-min sketch, we're keeping frequencies for each thing we've seen.

Conceptually, we use several hash functions.

It's similar to a bloom filter.
#+end_notes

** Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

Intuition for Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

#+attr_html: :width 600px
[[./count-min_sketch2.svg]]

insert(Bob)

#+begin_notes
Bob collides with Alice, but only on one of the hash functions.
#+end_notes

** Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

Intuition for Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

#+attr_html: :width 600px
[[./count-min_sketch3.svg]]

insert(Alice)

#+begin_notes
Later on, we see Alice again, and increment the values at each of the hash
positions.
#+end_notes

** Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

Intuition for Count-Min Sketch
#+REVEAL_HTML: <div class='vertspace2'></div>

#+attr_html: :width 600px
[[./count-min_sketch4.svg]]

how_many(Alice)?

#+begin_notes
Now we can ask, how many times have we seen Alice?

The min value of the hashes gives us an upper bound.

Once again, this is a monoid. In fact, it's a full abelian group. It's
commutative and it has an inverse - we know how to "erase" one of the times
we've seen Alice.
#+end_notes

** Monoidal Structure of Distributed Stats
#+REVEAL_HTML: <div class='vertspace2'></div>

Monoids pervade distributed computations, especially statistics.

 - closedness gives us bounded space
 - associativity unlocks the ability to stripe across hardware/time
 - identity value helps with ops

See also: Avi Bryant, /Add ALL the Things/ (Strange Loop 2013) \\
https://www.infoq.com/presentations/abstract-algebra-analytics

* Incremental Computation
#+REVEAL_HTML: <div class='vertspace2'></div>

Let's talk about processes evolving in time.

** Function composition is a monoid
#+REVEAL_HTML: <div class='vertspace2'></div>
We already saw an example of this...

#+begin_src c++
using computation_t = auto (*) (int) -> int;
using profile_data_t = std::pair<computation_t, chrono::nanoseconds>;
#+end_src

#+begin_src c++
using a_to_b = auto (*) (A) -> B;
using b_to_c = auto (*) (B) -> C;
#+end_src

#+begin_notes
Function composition is associative. It's closed.
What is the identity here? The identity function of course.

For now let's just restrict ourselves to thinking about functions from A to A.
(Endofunctions).
#+end_notes

** ~std::iota~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template<class ForwardIt, class T>
void iota(ForwardIt first, ForwardIt last, T value)
{
    while(first != last) {
        *first++ = value;
        ++value;
    }
}
#+end_src

A monoid lurks.

#+begin_notes
There is a lurking monoid here... let's try to write ~iota~ as an accumulate.
#+end_notes

** ~nonstd::iota~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template <typename ForwardIt, typename T>
void iota(ForwardIt first, ForwardIt last, T value) {
  std::accumulate(first, last, value, [](const auto &so_far, auto &next) {
    next = so_far;
    return so_far + 1;
  });
}
#+end_src

The structure revealed.

#+begin_notes
OK, so it's clearly a fold.

Actually ~std::accumulate~ musn't modify any of the elements of the range, and
here we're breaking that rule, so this is technically UB. But it's just an
engineering choice - we could trivially use our own version of accumulate (or
the ~accumulate_iter~ that we saw earlier).

Anyway, writing it this way abstracts the actual "+1" from the computational
structure.
#+end_notes

** ~nonstd::iota~
#+REVEAL_HTML: <div class='vertspace2'></div>
#+begin_src c++
template <typename ForwardIt, typename T, typename UnaryFunction>
void iota(ForwardIt first, ForwardIt last, T value, UnaryFunction f) {
  std::accumulate(first, last, value, [&](auto &so_far, auto &next) {
    next = so_far;
    return f(so_far);
  });
}
#+end_src

#+begin_notes
Once we separate the structure of the loop from what's actually happening to the
elements, it's easy to pull out the "+1" and have it be anything we want it to be.
#+end_notes

** ~nonstd::iterate~
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
template<class ForwardIt, class T, class EndoFunction>
constexpr void iterate(ForwardIt first, ForwardIt last, T init, EndoFunction f)
{
    while (first != last) {
        *first++ = init;
        init = f(std::move(init));
    }
}
// and of course iterate_n similarly

constexpr auto iota = [] (auto first, auto last, auto value) {
  iterate(first, last, value, [] (auto i) { return i + 1; });
};
#+end_src

#+begin_notes
And now we see iota revealed for what it really is: sometimes called an unfold.

`iterate` is a function that we don't have in the standard library, but it would
be really useful.

We could also look at it as a generalization of ~accumulate~, exposing the
internal state at each iteration, or as a kind of ~partial_sum~ that computes
the input as it goes.
#+end_notes

* Endofunctions and Procedural generation
#+REVEAL_HTML: <div class='vertspace2'></div>

Putting ~nonstd::iterate~ to work.

#+begin_notes
Endofunctions go hand-in-glove with procedural generation.
#+end_notes

** Maze generation
#+REVEAL_HTML: <div class='vertspace2'></div>
You probably know a few algorithms for maze generation.

 - Recursive backtracking
 - Prim's
 - Kruskal's
 - Aldous-Broder
 - Binary tree
 - Hunt-and-kill
 - Wilson's
 - Sidewinder
 - Eller's

#+begin_notes
Let's take maze generation as a simple example here. There are lots of different
algorithms for generating a minimum spanning tree, aka a maze.

You can look these up and implement them at your leisure. I'm just going to use
the last one as my example.

Eller's algorithm has a particular feature. Consider a simple 2D square maze.
Eller's algo generates the maze row by row, generating the next row from the
last one.
#+end_notes

** Eller's algorithm
#+REVEAL_HTML: <div class='vertspace2'></div>
Start with a row of unlinked cells, all in different sets

Then, given a row:
 - randomly link (east-west) adjacent cells from different sets, merge their sets
 - randomly link south at least once from each set of cells
 - any cells in the next row that were not linked from the north get new sets

To finish, link (east-west) all cells from different sets.

** Eller's algorithm
#+REVEAL_HTML: <div class='vertspace2'></div>

#+attr_html: :width 600px
[[./ellers.svg]]

#+begin_notes
1. you have a row. each cell in the row is notionally in a set.

2. you carve east (remove the walls) randomly between cells.

3. you carve south randomly at least once from each set. unconnected cells in the
new row belong to new sets.

4. go to 1 until you've done enough.

5. for the final row, carve east linking all the different sets.
#+end_notes

** Demo
#+REVEAL_HTML: <div class='vertspace2'></div>

Eller's algorithm: ~nonstd::iterate_n~ in action.

** Streaming with monoids
#+REVEAL_HTML: <div class='vertspace2'></div>
When we recognize a monoidal operation, and extract the state, we get
easier incremental computation ability.

* Monoid Homomorphisms
#+REVEAL_HTML: <div class='vertspace2'></div>

"A 25-dollar term for a 5-cent concept"

(thanks Kris)

** Changing one monoid into another

#+REVEAL_HTML: <div class='vertspace2'></div>
A /monoid homomorphism/ changes one monoid into another, e.g.


 - Strings form a monoid under concatenation

 - Integers form a monoid under addition

~string::length~ is a monoid homomorphism

 - the identity is preserved (empty string has length zero)
 - general structure is preserved
 - the monoids are different

** We do this all the time

#+REVEAL_HTML: <div class='vertspace2'></div>
It's very common that we do calculations in different spaces.

 - easier to think about
 - easier to calculate

#+begin_src c
main(n){float r,i,R,I,b;for(i=-1;i<1;i+=.06,puts(""))for(r=-2;I=i,(R=r)<1;
r+=.03,putchar(n+31))for(n=0;b=I*I,26>n++&&R*R+b<4;I=2*R*I+i,R=R*R-b+r);}
#+end_src

#+begin_notes
No reason to do a sqrt here.

Often times, we're using a monoid homomorphism.

See also:
 - dot product angle calculations (no need for inverse cosine)
 - logarithms (turn multiplication into addition)
 - fourier/laplace transforms (turn differential eqns into algebraic eqns)
#+end_notes

** Example

#+REVEAL_HTML: <div class='vertspace2'></div>
What's the best way to compute a fibonacci number?

#+begin_notes
I'm sure you know this one.
#+end_notes

** Fibonacci

#+REVEAL_HTML: <div class='vertspace2'></div>
The fibonacci sequence is a function:

#+REVEAL_HTML: <div class='vertspace2'></div>
\( \{fib_{n-1}, fib_n\} \rightarrow \{fib_n, fib_{n+1}\} \)

#+REVEAL_HTML: <div class='vertspace2'></div>
#+begin_src c++
using fib = auto (*)(std::pair<int, int>) -> std::pair<int, int>;
#+end_src

#+begin_notes
The fib sequence is a linear recurrence relation, which means we can model it
with a matrix that we raise to a power to compute the nth term.

What do we get out of this?

Function composition is a monoid, but we're stuck with computing it in linear
time, because we can't easily compute compositions of functions.

We transform that monoid into matrix multiplication - and we know how to
do that in logarithmic time.
#+end_notes

** Another example

#+REVEAL_HTML: <div class='vertspace2'></div>
A linear congruential PRNG is a function:

\( x_{n+1} = (ax_n + b) \mod m \)

#+REVEAL_HTML: <div class='vertspace2'></div>
Can we apply a similar transformation?

#+begin_notes
Like fibonacci, we can't do function composition in sublinear time.

But this is basically a monoid, and basically multiply with a bit of other stuff.
#+end_notes

** PRNG applications
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
std::linear_congruential_engine::discard(unsigned long long z);
#+end_src
"Advances the internal state by ~z~ times. Equivalent to calling ~operator()~
~z~ times and discarding the result."

#+REVEAL_HTML: <div class='vertspace2'></div>

#+ATTR_REVEAL: :frag appear
"For some engines, "fast jump" algorithms are known"

#+begin_notes
This could be useful in a few situations.

One example could be something like a particle system simulation that has
minimal state and spews out particles randomly. You could restore the state of
that system at an arbitrary point in time by fast-forwarding the RNG, and
assuming the particles produced have a maximum lifetime, you only need to
simulate a few frames to recover the complete state.
#+end_notes

** Logarithmic skipahead
#+begin_src c++
auto skip_rand = [](std::uint32_t x, int n) -> std::uint32_t {
  std::uint64_t G = x;
  std::uint64_t C = 0;
  {
    auto c = B;
    auto h = A;
    auto f = B;
    while (n > 0) {
      if (n & 1) {
        G = (G * h) % M;
        C = (C * h + f) % M;
      }
      f = (f * (h + 1)) % M;
      h = (h * h) % M;
      n >>= 1;
    }
  }
  return G + C;
};
#+end_src

#+begin_notes
Somewhat surprisingly this technique is not in libstdc++ or libc++.
There, ~discard~ does the obvious linear thing.
#+end_notes

** Fast discard
#+REVEAL_HTML: <div class='vertspace2'></div>

Modular exponentiation

#+REVEAL_HTML: <div class='vertspace2'></div>
/Random Number Generation with Arbitrary Strides/ -- Forrest B. Brown \\
https://laws.lanl.gov/vhosts/mcnp.lanl.gov/pdf_files/anl-rn-arb-stride.pdf

#+REVEAL_HTML: <div class='vertspace2'></div>
Also applies to other RNGs e.g.
 - PCG http://www.pcg-random.org/useful-features.html#jump-ahead-and-jump-back
 - xorshift https://arxiv.org/pdf/1404.0390.pdf

#+begin_notes
Maybe we didn't need to think in terms of monoids here, because we're used to
plain old arithmetic.

But thinking about the structure of the calculation in general can clue us in to
whether or not it's possible to transform it, and what are the likely gains if
we can.

Some RNGs (including PCG) can also be run backwards. Imagine the combination of
this with the maze generation we saw. We could fix (bound) the number of random
numbers used per row, make that the state, and get the ability to randomly
access any row of the maze.
#+end_notes

** Why use a MH?
#+REVEAL_HTML: <div class='vertspace2'></div>
When you spot a monoid, wonder if there's a monoid homomorphism.

Maybe you can get the calculation into a different space:
#+ATTR_REVEAL: :frag (appear)
 - where you can do more
 - where you can do things faster
 - where you can think more easily

#+begin_notes
All of these apply.
#+end_notes

* Even more on monoids
#+REVEAL_HTML: <div class='vertspace2'></div>

Things I don't have time to go into fully, \\
left as an exercise for the viewer.

#+REVEAL_HTML: <div class='vertspace2'></div>
When you start looking for monoids, they crop up /everywhere/.

** ಠ_ಠ
#+REVEAL_HTML: <div class='vertspace2'></div>
#+begin_src c++
template <class InputIt, class T, class BinaryOp>
T reduce(InputIt first, InputIt last, T init, BinaryOp binary_op);
#+end_src
"The behavior is non-deterministic if binary_op is not associative /or not
commutative./"

#+REVEAL_HTML: <div class='vertspace2'></div>
Data-level parallelism at war with function-level parallelism...

#+begin_notes
This is really unfortunate.

I understand that this is required for vectorization. But as we've seen,
associativity is the only crucial requirement for distributed calculations, and
the requirement for commutativity puts a spanner in the works for distribution.
#+end_notes

** Folding over tree structures
#+REVEAL_HTML: <div class='vertspace2'></div>

"Normal" ~accumulate~ has two operations:
 - one for combining with identity
 - one for combining with value

If we look at a tree structure as a sum type, we can extend this to:
 - one for combining each type of value

#+REVEAL_HTML: <div class='vertspace2'></div>
/std::accumulate: Exploring an Algorithmic Empire/ \\
https://www.youtube.com/watch?v=B6twozNPUoA

#+begin_notes
Quite often we handle treelike structures. It's much more likely then that we'll
fail to see the essential computation that's going on and fall back on a raw
loop because we think algorithms can't deal with what we're doing. If we think
in terms of monoids, we can get that separation of control flow from logic and
we can often use an accumulate-like algorithm to achieve that.
#+end_notes

** Folding over tree structures
#+REVEAL_HTML: <div class='vertspace2'></div>

#+begin_src c++
struct EmptyVector {};

template <typename T>
using Vector<T> = variant<EmptyVector, pair<T, Vector<T>>>;
#+end_src

#+begin_src c++
accumulate(first, last,
           init, // <- deal with EmptyVector
           plus) // <- deal with T
#+end_src

~nonstd::overload~ and the ability to have a recursive-lambda overload set
(e.g. http://wg21.link/p0847) helps.

#+begin_notes
P0847: deducing ~this~

We can achieve "recursive lambdas" by using variable template lambdas with
specializations.
#+end_notes

** Folding over tree structures
#+REVEAL_HTML: <div class='vertspace2'></div>
Q: How is folding over a tree different from ~std::visit~?

A: The same way ~accumulate~ is different from ~for_each~.

#+REVEAL_HTML: <div class='vertspace2'></div>
#+begin_src c++
const auto sum = std::accumulate(first, last, 0);

auto sum = 0;
std::for_each(first, last, [&] (auto value) { sum += value; };
#+end_src

#+begin_notes
With folds, we unlock a declarative code style. We remove the state from the
operation. It's easier to manage that state.

We get the idea of incremental computation again. Or distributed computation.
#+end_notes

** Futures as monoids
#+REVEAL_HTML: <div class='vertspace2'></div>

No, not as monads. Not today.

#+REVEAL_HTML: <div class='vertspace2'></div>
Futures form a monoid with the "race" operation. (~when_any~)
 - (A 'race' B) 'race' C == A 'race' (B 'race' C)
 - the identity is ~never~ (the future that never completes)

#+begin_notes
Explain.

This could be useful for example for modelling cancellation. Imagine "racing" a
computation against a UI element that completes its future when the user hits
cancel...
#+end_notes

** Parsers as monoids
#+REVEAL_HTML: <div class='vertspace2'></div>
You remember:

"A parser for things is a function from strings to lists of pairs of things and
strings."

#+REVEAL_HTML: <div class='vertspace2'></div>
Parsers form a monoid under alternation.
 - (A | B) | C == A | (B | C)
 - the identity is the parser that always fails

#+REVEAL_HTML: <div class='vertspace2'></div>
/~constexpr~ ALL the things!/ \\
https://www.youtube.com/watch?v=HMB9oXFobJc

#+begin_notes
Jason and I used this in our 2017 talk, with a parser combinator approach to
building compile-time UDLs.

Alternation can be used to provide error messages (the identity parser) in such
a scheme.
#+end_notes

** Training sets as monoids

#+REVEAL_HTML: <div class='vertspace2'></div>
You have a large set of data to train on.

Maybe you have a monoid.

Train on large set => produce distribution.

Train on incremental data => produce distribution.

Can you combine the distributions monoidally?

https://izbicki.me/blog/gausian-distributions-are-monoids.html


#+begin_notes
I know very little about machine learning, but I'm sure monoids apply.

Recall: one of the things that monoids unlock is distributed computation. When
you're running across very large data sets that is invaluable.
#+end_notes

** Incremental regular expression matching with monoids

#+REVEAL_HTML: <div class='vertspace2'></div>
You have:
 - a regular expression
 - a string to match

Perform the match once, then edit the string.

How expensive is performing a second match?

http://blog.sigfpe.com/2009/01/fast-incremental-regular-expression.html

#+begin_notes
Monoids give you incremental computation.

Dan Piponi wrote about this on his blog.
"You can do this in C++, say, using mutable red-black trees."
#+end_notes

** How to impress Haskell programmers
#+REVEAL_HTML: <div class='vertspace2'></div>
Play The Monoid Game...

#+REVEAL_HTML: <div class='vertspace2'></div>
You: "X is a monoid!"

Haskeller, impressed: "Ooh, you mean in the sense of [FP stuff]?"

You: "Up to isomorphism, yes."

#+begin_notes
This works because /practically everything/ is a monoid under some
interpretation.
#+end_notes

* Final thoughts
#+REVEAL_HTML: <div class='vertspace2'></div>
 - thinking about structure helps to separate control flow from logic
 - monoids are a ubiquitous pattern for doing that
 - try to think beyond just numerics
 - added benefit: distributed and/or incremental computation

#+REVEAL_HTML: <div class='vertspace2'></div>
#+REVEAL_HTML: <blockquote nil><p>&quot;Discovery consists of seeing what everybody<br>
#+REVEAL_HTML: has seen, and thinking what nobody has thought.&quot;</p>
#+REVEAL_HTML: <div></div><div class='author'>-- Albert Szent-Györgyi (Nobel Laureate in Medicine, 1937)</div></blockquote>

#+begin_notes
I hope that after this talk you can look at your code in a new way and think
what you have not thought before.
#+end_notes


* Notes                                                            :noexport:
** Endofunctions

Functions from A to A
Processes evolving in time
std::iota
std::iterate
ranges

Examples:

 - Eller's algo for maze generation - plain std::accumulate (linear data dependence)
   or partial_sum for intermediate output (good range example?)

 - RNG - LCG is a linear recurrence relation f :: a -> a
   represent as a matrix -> function composition is raising to nth power (log n)
   can "fast forward" RNG in log time because it's a monoid
https://www.nayuki.io/page/fast-skipping-in-a-linear-congruential-generator

Research:

https://meetingcpp.com/blog/items/ranges-for-numerical-problems-402.html
https://www.youtube.com/watch?v=13r9QY6cmjc
http://people.math.gatech.edu/~ecroot/recurrence_notes2.pdf

** Futures

when_any and never
UI applications

** Stronger than monoids
Commutativity
Existence of an inverse

** Accumulate vs reduce

Reduce requires commutativity for vectorization
Data-level parallelism at war with function-level parallelism
(parallelism vs concurrency)

** Balanced reduction

** Incremental computation

** Bigger applications

** Serialization
Monoid-like, but with varying types

** Profiling
Another kind of serialization

** Statistics
Keeping a mean
Keeping a median
Top n
Histograms

** Probabilistic algorithms
"Fantastic algorithms and where to find them"
"Add ALL the things!"

Hyperloglog
Count min-sketch

** Config

JSON objects, databases, configuration blobs, sets of command-line flags
Protocol buffers

We "reinvent" monoids all the time without realizing it! Most of the time when
we deal with these kinds of things, we don't think about their monoidal nature.

https://mail.haskell.org/pipermail/haskell-cafe/2009-January/053709.html

** Parsers

Parsers are monoids under alternation. The identity is the parser that always
fails. This is a common pattern if you have an operation that can fail.

cf. when_any
cf. optional

** Monoid homomorphisms

A function that preserves the monoid structure. If A and B are monoids under
some operations, then f :: A -> B is a monoid homomorphism if it preserves the
structure.

e.g.
 - strings are monoids under concatenation.
 - integers are monoids under addition.

string length is a monoid homomorphism.

** Why use monoid homomorphisms?

- to get into a space that is easier to reason about
- to be able to do more
- for performance
- all of the above

** For perf?

We're always doing things for performance reasons of course.
We're often computing things in a "different space" for perf reasons.

e.g.

In vector spaces, we can avoid square roots when computing magnitudes because we
can compare to a precomputed magnitude in "squared space" instead.

In vector spaces, we measure angles between vectors by comparing with
precomputed cosine constants rather than doing an inverse trig function.

** Ofuscated example

#+begin_src c
main(n){float r,i,R,I,b;for(i=-1;i<1;i+=.06,puts(""))for(r=-2;I=i,(R=r)<1;
r+=.03,putchar(n+31))for(n=0;b=I*I,26>n++&&R*R+b<4;I=2*R*I+i,R=R*R-b+r);}
#+end_src

#+begin_notes
We're so used to working in "a more computationally efficient space".

Maybe you can guess what this does? Hint: it's from the mid-90s.

Check out the "R*R+b<4" part.
#+end_notes

** Monoid Homomorphism example

The usual example is string -> int (length).

Sometimes the monoid is buried. Often the "surface" monoid is a monoid we can't
express very well in C++. Like function composition.

LCG example.

** Regular expressions

http://blog.sigfpe.com/2009/01/fast-incremental-regular-expression.html

** Tournaments

** Diagrams

** My favourite quote

"Discovery consists of seeing what everybody has seen, and thinking what nobody
has thought."

Albert Szent-Györgyi. (Hungarian Nobel Laureate in Medicine, 1937)

#+begin_notes
I hope that after this talk you can look at your code in a new way and think
what you have not thought before.
#+end_notes

** References
/Cultivating Instinct/ Katrina Owen
https://www.youtube.com/watch?v=Q1Tlo4VnQrA
